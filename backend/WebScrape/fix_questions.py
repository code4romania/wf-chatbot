import json
import os
import re
# Assuming GeminiChat is available from a local Gemini.py file
# You should replace this with your actual GeminiChat implementation
from Gemini import GeminiChat

# --- Configuration ---
# Directory where the questions generated by the previous script are located
INPUT_QUESTIONS_ROOT = "./data_whole_page/dopomoha_general_simple"
# Directory where the original scraped data with summaries is located
ORIGINAL_SUMMARY_ROOT = "./data_whole_page/dopomoha_stripped"
# Directory where the fixed questions will be saved
OUTPUT_FIXED_ROOT = "./data_whole_page/dopomoha_fixed_questions"
# Path for logging failures during the fixing process
FAILURES_FILE = os.path.join(OUTPUT_FIXED_ROOT, "notes", "fix_fails.json")

# Define the batch size for Gemini API calls
BATCH_SIZE = 10 # Adjust this value based on API token limits and desired efficiency

# Ensure output directories exist
os.makedirs(OUTPUT_FIXED_ROOT, exist_ok=True)
os.makedirs(os.path.join(OUTPUT_FIXED_ROOT, "notes"), exist_ok=True)

# Initialize the GeminiChat instance
chat = GeminiChat()

def load_json_file(path):
    """Loads a JSON file and returns its content, or None if an error occurs."""
    if os.path.exists(path):
        with open(path, 'r', encoding='utf-8') as f:
            try:
                return json.load(f)
            except json.JSONDecodeError as e:
                print(f"Error decoding JSON from {path}: {e}")
                return None
    print(f"File not found: {path}")
    return None

def save_json_file(data, path):
    """Saves data to a JSON file."""
    os.makedirs(os.path.dirname(path), exist_ok=True) # Ensure directory exists before saving
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_failures():
    """Loads existing failures from the log file."""
    return load_json_file(FAILURES_FILE) or []

def save_failures(failures):
    """Saves failures to the log file."""
    save_json_file(failures, FAILURES_FILE)

def get_summary_for_content_block(page_name, content_block_id):
    """
    Loads the original scraped data for a given page and finds the summary
    associated with a specific content_block_id.
    """
    original_scraped_path = os.path.join(ORIGINAL_SUMMARY_ROOT, f"{page_name}.json")
    scraped_data = load_json_file(original_scraped_path)
    if scraped_data:
        for entry in scraped_data:
            if entry.get('id') == content_block_id:
                return entry.get('summary')
    return None

def fix_questions_batch(batch_data, verbose=False):
    """
    Uses the Gemini API to rephrase a batch of questions to be context-independent.
    `batch_data` is a list of dictionaries, where each dict contains:
    {'question_id': ..., 'summary': '...', 'original_question': '...'}
    """
    if not batch_data:
        return {} # Return empty dict if batch is empty

    # Prepare the input for the Gemini prompt
    input_for_gemini = []
    for item in batch_data:
        input_for_gemini.append({
            "question_id": item['question_id'],
            "summary": item['summary'],
            "original_question": item['original_question']
        })

    prompt = (
        f"Given the following JSON list of content summaries and their corresponding questions, "
        f"rephrase each 'original_question' to be *completely standalone*, *general*, and *open-ended*. "
        f"Each rephrased question must start with 'What', 'How', or 'When'. "
        f"It should *not* use words like 'this', 'these', 'summary', or 'content'. "
        f"If an 'original_question' is already good and meets all criteria, just return it as is. "
        f"SUPER IMPORTANT: Return only a valid JSON list of objects. Each object must have "
        f"'question_id' (matching the input) and 'fixed_question' (the rephrased question). "
        f"DO NOT include any additional text or formatting outside the JSON list.\n\n"
        f"Input:\n```json\n{json.dumps(input_for_gemini, indent=2)}\n```\n\n"
        f"Output:\n```json\n"
    )

    history = []
    fixed_questions_map = {} # To store {question_id: fixed_question}

    for i in range(3): # Attempt up to 3 times to get a valid JSON response
        resp, _ = chat.send(prompt) # Assuming chat.send returns (response_text, other_info)
        history.append(('user', prompt))
        history.append(('assistant', resp))

        # Attempt to parse the JSON response
        try:
            # Look for a JSON block in the response
            match = re.search(r"```json\s*(\[.*?\])\s*```", resp, re.DOTALL)
            if match:
                json_str = match.group(1)
            else:
                # If no code block, try to parse the whole response as JSON
                json_str = resp.strip()

            parsed_response = json.loads(json_str)

            if isinstance(parsed_response, list):
                for item in parsed_response:
                    if isinstance(item, dict) and 'question_id' in item and 'fixed_question' in item:
                        # Basic validation for the fixed question format
                        fixed_q_text = item['fixed_question'].strip()
                        if fixed_q_text.endswith('?') and \
                           (fixed_q_text.startswith('What') or fixed_q_text.startswith('How') or fixed_q_text.startswith('When')):
                            fixed_questions_map[item['question_id']] = fixed_q_text
                        elif verbose:
                            print(f"  Warning: Fixed question for QID {item['question_id']} ('{fixed_q_text}') did not meet format criteria.")
                    elif verbose:
                        print(f"  Warning: Malformed item in batch response: {item}")
                if fixed_questions_map: # If at least one question was successfully parsed and validated
                    return fixed_questions_map
            elif verbose:
                print(f"  Attempt {i+1}: Parsed response is not a list: {parsed_response}")

        except json.JSONDecodeError as e:
            if verbose:
                print(f"  Attempt {i+1}: JSON parsing failed - {e}. Raw response: {resp}")
        except Exception as e:
            if verbose:
                print(f"  Attempt {i+1}: An unexpected error occurred during parsing - {e}. Raw response: {resp}")

        # If parsing failed or no valid questions were extracted, prepare a corrective prompt
        correction = (
            f"Your previous response could not be parsed as a valid JSON list of objects "
            f"with 'question_id' and 'fixed_question' keys, or the questions did not meet format criteria. "
            f"Please ensure you return *only* a valid JSON list literal, as specified in the original prompt. "
            f"Example format: ```json\n[{{\"question_id\": 1, \"fixed_question\": \"How can one apply for aid?\"}}]\n```"
        )
        prompt = correction # Update the prompt for the next iteration
        if verbose:
            print(f"  Correction prompt for batch: {correction}")

    return fixed_questions_map # Return whatever was successfully parsed, or empty if all failed

def process_questions_file(lang_code, page_name, verbose=False):
    """
    Reads a questions JSON file, collects questions into batches,
    attempts to fix each batch, and saves the updated file to the output directory.
    """
    input_path = os.path.join(INPUT_QUESTIONS_ROOT, lang_code, f"{page_name}.json")
    output_dir = os.path.join(OUTPUT_FIXED_ROOT, lang_code)
    os.makedirs(output_dir, exist_ok=True) # Ensure output directory for language exists
    output_path = os.path.join(output_dir, f"{page_name}.json")

    questions_data = load_json_file(input_path)
    if not questions_data or 'questions' not in questions_data:
        if verbose:
            print(f"Skipping {input_path}: Invalid or empty questions data.")
        return

    fixed_questions_count = 0
    failures = load_failures() # Load existing failures for global logging
    updated_questions = []

    print(f"  Processing questions in {page_name}.json...")

    questions_to_process = []
    for q_entry in questions_data['questions']:
        original_question = q_entry['question']
        content_block_id = q_entry['content_block_id']
        question_id = q_entry['question_id']

        summary = get_summary_for_content_block(page_name, content_block_id)

        if summary is None:
            failures.append({
                'file': f"{lang_code}/{page_name}.json",
                'question_id': question_id,
                'original_question': original_question,
                'reason': f"Could not find summary for content_block_id: {content_block_id} in original data."
            })
            updated_questions.append(q_entry) # Keep original question if context is missing
            if verbose:
                print(f"  FAILED: Summary not found for {page_name}, content block {content_block_id}. Question QID {question_id} skipped for fixing.")
            continue

        questions_to_process.append({
            'question_id': question_id,
            'summary': summary,
            'original_question': original_question,
            'original_q_entry_ref': q_entry # Keep a reference to the original entry for direct update
        })

    # Process questions in batches
    for i in range(0, len(questions_to_process), BATCH_SIZE):
        batch = questions_to_process[i:i + BATCH_SIZE]
        if verbose:
            print(f"  Sending batch of {len(batch)} questions to Gemini...")

        fixed_questions_map = fix_questions_batch(batch, verbose)

        for item in batch:
            q_entry_ref = item['original_q_entry_ref']
            question_id = item['question_id']
            original_question = item['original_question']
            summary_context = item['summary']

            if question_id in fixed_questions_map:
                fixed_q = fixed_questions_map[question_id]
                q_entry_ref['question'] = fixed_q # Update the question with the fixed version
                fixed_questions_count += 1
                if verbose:
                    print(f"  Fixed QID {question_id}: '{original_question}' -> '{fixed_q}'")
            else:
                # Log failure if the question was in the batch but not returned as fixed
                failures.append({
                    'file': f"{lang_code}/{page_name}.json",
                    'question_id': question_id,
                    'original_question': original_question,
                    'summary_context': summary_context,
                    'reason': "Question was in batch but not successfully fixed/returned by Gemini."
                })
                if verbose:
                    print(f"  FAILED to fix QID {question_id}: '{original_question}'. Keeping original.")

            updated_questions.append(q_entry_ref) # Add the (potentially fixed) question to the list

    questions_data['questions'] = updated_questions
    save_json_file(questions_data, output_path) # Save the updated questions to the new path
    save_failures(failures) # Save the updated failures log
    print(f"  Finished processing {len(questions_data['questions'])} questions in {page_name}.json for {lang_code}. Fixed {fixed_questions_count} questions. Failures logged to {FAILURES_FILE}.")


if __name__ == '__main__':
    # Iterate through each language subdirectory in the input questions root
    for lang_code in os.listdir(INPUT_QUESTIONS_ROOT):
        lang_dir = os.path.join(INPUT_QUESTIONS_ROOT, lang_code)
        if os.path.isdir(lang_dir): # Ensure it's a directory
            print(f"Processing language: {lang_code}")
            # Iterate through each JSON file within the language directory
            for filename in os.listdir(lang_dir):
                if filename.endswith('.json'):
                    page_name, _ = os.path.splitext(filename)
                    print(f"Processing file: {filename}")
                    process_questions_file(lang_code, page_name, verbose=True) # Set verbose to True for detailed console output
            print(f"Finished processing all files for language: {lang_code}.")

